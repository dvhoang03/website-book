import os
import json
from fastapi import FastAPI
from pydantic import BaseModel
from dotenv import load_dotenv
from sqlalchemy import create_engine, text
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_community.utilities import SQLDatabase
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_core.messages import HumanMessage, AIMessage

# Load bi·∫øn m√¥i tr∆∞·ªùng
load_dotenv()
app = FastAPI()

# 1. C·∫•u h√¨nh Database & AI
db_uri = os.getenv("DATABASE_URL")
db = SQLDatabase.from_uri(db_uri, sample_rows_in_table_info=0)
api_key = os.getenv("GOOGLE_API_KEY")
engine = create_engine(db_uri)

# Model ch√≠nh
llm = ChatGoogleGenerativeAI(
    model="gemini-2.0-flash",
    google_api_key=api_key,
    temperature=0,
    convert_system_message_to_human=True
)

embeddings = GoogleGenerativeAIEmbeddings(
    model="models/text-embedding-004",
    google_api_key=api_key,
    task_type="retrieval_query"
)

class ChatRequest(BaseModel):
    question: str
    session_id: str

# ==============================================================================
# MODULE: QU·∫¢N L√ù L·ªäCH S·ª¨ CHAT (MEMORY)
# ==============================================================================
def get_chat_history(session_id: str, limit=6):
    """L·∫•y 6 tin nh·∫Øn g·∫ßn nh·∫•t t·ª´ b·∫£ng chat_messages"""
    try:
        with engine.connect() as conn:
            # Gi·∫£ s·ª≠ b·∫£ng chat_messages c√≥ c·ªôt: role (user/bot), content, conversationId (session_id)
            # B·∫°n c·∫ßn ƒëi·ªÅu ch·ªânh c√¢u query n√†y kh·ªõp v·ªõi t√™n c·ªôt th·ª±c t·∫ø trong DB NestJS c·ªßa b·∫°n
            query = text(f"""
                SELECT role, content 
                FROM chat_messages 
                WHERE "conversationId" = :session_id 
                ORDER BY "created_at" DESC LIMIT :limit
            """)
            result = conn.execute(query, {"session_id": session_id, "limit": limit}).fetchall()
            
            messages = []
            # ƒê·∫£o ng∆∞·ª£c l·∫°i ƒë·ªÉ ƒë√∫ng th·ª© t·ª± th·ªùi gian (C≈© -> M·ªõi)
            for row in reversed(result):
                if row[0] == 'user':
                    messages.append(HumanMessage(content=row[1]))
                else:
                    messages.append(AIMessage(content=row[1]))
            return messages
    except Exception as e:
        print(f"‚ö†Ô∏è L·ªói l·∫•y l·ªãch s·ª≠: {e}")
        return []

def save_chat_message(session_id: str, role: str, content: str):
    """L∆∞u tin nh·∫Øn v√†o DB (NestJS Backend th∆∞·ªùng l√†m vi·ªác n√†y, nh∆∞ng Python c≈©ng c√≥ th·ªÉ l∆∞u ph·ª•)"""
    # Trong ki·∫øn tr√∫c Microservice c·ªßa b·∫°n, t·ªët nh·∫•t l√† ƒë·ªÉ NestJS l∆∞u message tr∆∞·ªõc khi g·ªçi Python.
    # H√†m n√†y ƒë·ªÉ demo logic th√¥i.
    pass 

# ==============================================================================
# C√îNG C·ª§ 1: ROUTER (PH√ÇN LO·∫†I C√ÇU H·ªéI - C√ì CONTEXT)
# ==============================================================================
def route_question(question: str, history: list):
    # Chuy·ªÉn history th√†nh text ƒë·ªÉ Router hi·ªÉu ng·ªØ c·∫£nh
    history_text = "\n".join([f"{m.type}: {m.content}" for m in history])
    
    prompt = ChatPromptTemplate.from_template("""
    L·ªãch s·ª≠ tr√≤ chuy·ªán:
    {history}
    
    C√¢u h·ªèi hi·ªán t·∫°i: {question}
    
    H√£y ph√¢n lo·∫°i c√¢u h·ªèi v√†o 1 trong 3 lo·∫°i:
    1. "DATABASE": H·ªèi v·ªÅ th√¥ng tin s√°ch (gi√°, t√°c gi·∫£, s·ªë l∆∞·ª£ng, li·ªát k√™).
    2. "POLICY": H·ªèi ch√≠nh s√°ch chung (ƒë·ªïi tr·∫£, giao h√†ng).
    3. "CHITCHAT": Ch√†o h·ªèi, c·∫£m ∆°n, ho·∫∑c n√≥i chuy·ªán ti·∫øp n·ªëi l·ªãch s·ª≠ m√† kh√¥ng c·∫ßn tra c·ª©u.
    
    Ch·ªâ tr·∫£ v·ªÅ ƒë√∫ng 1 t·ª´: DATABASE ho·∫∑c POLICY ho·∫∑c CHITCHAT.
    """)
    chain = prompt | llm | StrOutputParser()
    return chain.invoke({"question": question, "history": history_text}).strip()

# ==============================================================================
# C√îNG C·ª§ 2: X·ª¨ L√ù DATABASE
# ==============================================================================
def handle_database_query(question: str, history: list):
    history_text = "\n".join([f"{m.type}: {m.content}" for m in history])
    
    # B∆∞·ªõc 1: Vi·∫øt SQL (C√≥ tham kh·∫£o l·ªãch s·ª≠ ƒë·ªÉ hi·ªÉu t·ª´ "n√≥", "cu·ªën ƒë√≥")
    sql_prompt = ChatPromptTemplate.from_template("""
    B·∫°n l√† chuy√™n gia SQL.
    L·ªãch s·ª≠: {history}
    C√¢u h·ªèi: {question}
    
    Schema: {schema}
    
    Nhi·ªám v·ª•: Vi·∫øt c√¢u l·ªánh PostgreSQL query.
    L∆∞u √Ω: 
    - N·∫øu c√¢u h·ªèi l√† "gi√° bao nhi√™u", h√£y t√¨m gi√° c·ªßa cu·ªën s√°ch ƒë∆∞·ª£c nh·∫Øc ƒë·∫øn g·∫ßn nh·∫•t trong l·ªãch s·ª≠.
    - D√πng ILIKE cho t√™n s√°ch. T√™n b·∫£ng trong ngo·∫∑c k√©p.
    - Ch·ªâ tr·∫£ v·ªÅ SQL.
    """)
    
    schema_info = db.get_table_info(['books', 'authors'])
    sql_chain = sql_prompt | llm | StrOutputParser()
    generated_sql = sql_chain.invoke({"question": question, "history": history_text, "schema": schema_info})
    
    cleaned_sql = generated_sql.replace("```sql", "").replace("```", "").strip()
    print(f"Generated SQL: {cleaned_sql}")

    # B∆∞·ªõc 2: Ch·∫°y SQL
    try:
        with engine.connect() as conn:
            # Ch·∫∑n c√°c l·ªánh nguy hi·ªÉm
            if "DROP" in cleaned_sql.upper() or "DELETE" in cleaned_sql.upper():
                return "T√¥i kh√¥ng ƒë∆∞·ª£c ph√©p th·ª±c hi·ªán l·ªánh n√†y."
                
            result = conn.execute(text(cleaned_sql)).fetchall()
            if not result:
                return "Xin l·ªói, kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu ph√π h·ª£p."
            
            # B∆∞·ªõc 3: Tr·∫£ l·ªùi t·ª± nhi√™n
            final_prompt = ChatPromptTemplate.from_template("""
            C√¢u h·ªèi: {question}
            K·∫øt qu·∫£ DB: {result}
            
            H√£y tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n k·∫øt qu·∫£ DB. Gi·ªçng ƒëi·ªáu th√¢n thi·ªán nh√¢n vi√™n b√°n h√†ng.
            """)
            chain = final_prompt | llm | StrOutputParser()
            return chain.invoke({"question": question, "result": str(result)})
            
    except Exception as e:
        print(f"SQL Error: {e}")
        return "Xin l·ªói, h·ªá th·ªëng kh√¥ng t√¨m th·∫•y th√¥ng tin."

# ==============================================================================
# C√îNG C·ª§ 3: X·ª¨ L√ù CH√çNH S√ÅCH
# ==============================================================================
def handle_policy_query(question: str):
    try:
        vector = embeddings.embed_query(question)
        with engine.connect() as conn:
            query = text("""
                SELECT content, 1 - (embedding <=> CAST(:vector AS vector)) as similarity 
                FROM policies 
                ORDER BY similarity DESC LIMIT 1
            """)
            row = conn.execute(query, {"vector": str(vector)}).fetchone()
            
            if row and row[1] > 0.3:
                return row[0] # Tr·∫£ v·ªÅ n·ªôi dung policy
            else:
                return "Xin l·ªói, t√¥i kh√¥ng t√¨m th·∫•y ch√≠nh s√°ch li√™n quan."
    except Exception:
        return "ƒêang b·∫£o tr√¨ t√≠nh nƒÉng tra c·ª©u."

# ==============================================================================
# API ENDPOINT
# ==============================================================================
@app.post("/chat")
async def chat_endpoint(req: ChatRequest):
    print(f"\nüí¨ User asking: {req.question}")
    
    # 1. L·∫•y l·ªãch s·ª≠ chat t·ª´ DB (Quan tr·ªçng!)
    history = get_chat_history(req.session_id)
    
    # 2. Ph√¢n lo·∫°i c√¢u h·ªèi
    intent = route_question(req.question, history)
    print(f"üëâ Intent: {intent}")
    
    response = ""
    source = "ai"
    
    if intent == "DATABASE":
        response = handle_database_query(req.question, history)
        source = "database"
    elif intent == "POLICY":
        response = handle_policy_query(req.question)
        source = "policy"
    else:
        # Chitchat: Tr·∫£ l·ªùi d·ª±a tr√™n l·ªãch s·ª≠
        prompt = ChatPromptTemplate.from_template("""
        L·ªãch s·ª≠: {history}
        Ng∆∞·ªùi d√πng: {question}
        H√£y tr·∫£ l·ªùi th√¢n thi·ªán, ng·∫Øn g·ªçn.
        """)
        chain = prompt | llm | StrOutputParser()
        response = chain.invoke({"history": history, "question": req.question})
        source = "chitchat"

    return {"source": source, "content": response}